{"nbformat_minor": 1, "cells": [{"source": "# Predict chat reason using IBM Watson Machine Learning", "cell_type": "markdown", "metadata": {}}, {"source": "This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n\nSome familiarity with Python is helpful. This notebook uses Python 2.0 and Apache\u00ae Spark 2.0.\n\n\n## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Load a CSV file into an Apache\u00ae Spark DataFrame.\n-  Explore data.\n-  Prepare data for training and evaluation.\n-  Create an Apache\u00ae Spark machine learning pipeline.\n-  Train and evaluate a model.\n-  Persist a pipeline and model in Watson Machine Learning repository.\n-  Deploy a model for online scoring using Wastson Machine Learning API.\n-  Score sample scoring data using the Watson Machine Learning API.\n\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1.\t[Setup](#setup)\n2.\t[Load and explore data](#load)\n3.\t[Create spark ml model](#model)\n4.\t[Persist model](#save)\n5.\t[Predict locally and visualize](#predict)\n6.\t[Deploy and score in a Cloud](#deploy)\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"setup\"></a>\n## 1. Setup\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n-  Upload **cox.csv** data as a data asset in IBM Data Science Experience.\n-  Make sure that you are using a Spark 2.0 kernel.\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"load\"></a>\n## 2.  Load and explore data", "cell_type": "markdown", "metadata": {}}, {"source": "IBM Data Science Experience (DSX) makes it easy to load your files with a few clicks!", "cell_type": "markdown", "metadata": {}}, {"source": "**Action**: Import the data and add .option('inferSchema','true)", "cell_type": "markdown", "metadata": {}}, {"source": "import ibmos2spark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# @hidden_cell\n# The following code is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\nproperties_3120d808e7fd4d15a725a507f90ae16c = {\n    'jdbcurl': 'jdbc:db2://',\n    'user': 'bluadmin',\n    'password': ''\n}\n\ndata_df_1 = spark.read.jdbc(properties_3120d808e7fd4d15a725a507f90ae16c['jdbcurl'], table='CHANGE_SCHEMA_NAME.TRAINING', properties=properties_3120d808e7fd4d15a725a507f90ae16c)\ndata_df_1.head()\n\n", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Explore the loaded data by using the following Apache\u00ae Spark DataFrame methods:\n-  print schema\n-  count all the records\n-  print top five records", "cell_type": "markdown", "metadata": {}}, {"source": "df = data_df_1\n\ndf.printSchema()\nprint \"# of records: \" + str(df.count())", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "We can see that there are 1165 rows and we have 7 fields we will use to predict the title (label) of the movie.", "cell_type": "markdown", "metadata": {}}, {"source": "df.show(5)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Top 5 rows", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"model\"></a>\n## 3. Create an Apache Spark machine learning model\n\nIn this section we will prepare data, create an Apache Spark machine learning pipeline, and train a model.\n\n\n### 3.1:  Prepare data\n\nIn this subsection we will split our data into: training, test, and predict datasets.", "cell_type": "markdown", "metadata": {}}, {"source": "split_data = df.randomSplit([0.9, 0.1], 24)\n\ntraining_data = df\ntest_data = split_data[1]\n\nprint \"Training records: \" + str(training_data.count())\nprint \"Test records: \" + str(test_data.count())", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "As you can see our data has been successfully split into three datasets: \n\n-  The training dataset, which is the largest group, is used for training.\n-  The test dataset will be used for model evaluation and is used to test the assumptions of the model.\n-  The predict dataset will be used for prediction.", "cell_type": "markdown", "metadata": {}}, {"source": "### 3.2:  Create pipeline and train a model\n\nIn this section we create an Apache Spark machine learning pipeline and then train the model.\n\nFirst we need to import several packages that will be used in the next few steps.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "First we need to convert all the string fields to numeric values.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "categoricalColumns = [\"KEY1\", \"KEY2\", \"KEY3\", \"KEY4\", \"KEY5\", \"KEY6\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  stringIndexer.setHandleInvalid(\"skip\")\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  stages += [stringIndexer, encoder]\nstringIndexer_label = StringIndexer(inputCol=\"CLASSIFICATION\", outputCol=\"label\").fit(df)\nstages += [stringIndexer_label]\n\n", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Create a feature vector by combining all features together.", "cell_type": "markdown", "metadata": {}}, {"source": "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns)\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Next we define a Random Forest estimator.", "cell_type": "markdown", "metadata": {}}, {"source": "rf = NaiveBayes(labelCol=\"label\", featuresCol=\"features\",smoothing=.1)\nstages += [rf]", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Next we convert the indexed labels back to the original label.", "cell_type": "markdown", "metadata": {}}, {"source": "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)\nstages += [labelConverter]", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now we will put all the steps into a pipeline. ", "cell_type": "markdown", "metadata": {}}, {"source": "pipeline_rf = Pipeline(stages=stages)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now we will create a model using our pipeline and the training_data dataset.", "cell_type": "markdown", "metadata": {}}, {"source": "model_rf = pipeline_rf.fit(training_data)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now we will check our model accuracy using our test_data dataset.", "cell_type": "markdown", "metadata": {}}, {"source": "predictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "At this point we would tune the model for desired accuracy, for this example we will move on.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"save\"></a>\n## 4. Persist model in IBM Watson Machine Learning", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using python client libraries.\n\nFirst, you must import client libraries.\n\n**Note**: Apache Spark 2.0 or higher is required.", "cell_type": "markdown", "metadata": {}}, {"source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Authenticate to Watson Machine Learning service on Bluemix.\n\n**Action**: Use your Watson Machine Learning service instance credentials below.\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "username = ''\npassword = ''\nservice_path = 'https://ibm-watson-ml.mybluemix.net'\ninstance_id = ''", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. ", "cell_type": "markdown", "metadata": {}}, {"source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Create model artifact (abstraction layer)", "cell_type": "markdown", "metadata": {}}, {"source": "model_artifact = MLRepositoryArtifact(model_rf, training_data=training_data, name=\"NLU Demo\")", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).", "cell_type": "markdown", "metadata": {}}, {"source": "### 4.1: Save pipeline and model", "cell_type": "markdown", "metadata": {}}, {"source": "saved_model = ml_repository_client.models.save(model_artifact)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Get saved model metadata from Watson Machine Learning using the meta.available_props() method.", "cell_type": "markdown", "metadata": {}}, {"source": "saved_model.meta.available_props()", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "**Tip**:  **modelVersionHref** is our model unique id in Watson Machine Learning.", "cell_type": "markdown", "metadata": {}}, {"source": "print saved_model.meta.prop(\"modelVersionHref\")", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "### 4.2: Load model\n\nNow that we saved the model we will load it and verify the name.", "cell_type": "markdown", "metadata": {}}, {"source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "print str(loadedModelArtifact.name)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "<a id=\"predict\"></a>\n## 5. Predict locally and visualize\n\nIn this section we will score test data using the loaded model.", "cell_type": "markdown", "metadata": {}}, {"source": "### 5.1: Make local prediction using loaded model and predict data", "cell_type": "markdown", "metadata": {}}, {"source": "predictions = loadedModelArtifact.model_instance().transform(test_data)", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "predictions.show(3)\n", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "<a id=\"deploy\"></a>\n## 6. Deploy and create online scoring endpoint", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \nFor more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/).", "cell_type": "markdown", "metadata": {}}, {"source": "To work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:", "cell_type": "markdown", "metadata": {}}, {"source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v3/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now that we have the token we can create an online scoring endpoint.\n\nFirst we will check the model for existing deployments and get the deployments url, then we will create the online deployment.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "published_model_details = service_path + \"/v3/wml_instances/\" + instance_id + \"/published_models/\"\\\n+ loadedModelArtifact.uid \nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get_model_details = requests.get(published_model_details, headers=header)\n\nprint 'Existing deployment count: ' + str(json.loads(response_get_model_details.text).get('entity').get('deployments').get('count'))\ndeployments_endpoint = json.loads(response_get_model_details.text).get('entity').get('deployments').get('url')\nprint deployments_endpoint", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now take the scoring endpoint and the list of Labels and update the node red flow.", "cell_type": "markdown", "metadata": {}}, {"source": "payload_online_endpoint = {\"name\": \"COX Prediction Deployment\", \"description\": \"NLU prediction endpoint\", \"type\": \"online\"}\nresponse_online = requests.post(deployments_endpoint, json=payload_online_endpoint, headers=header)\n\nscoring_endpoint = json.loads(response_online.text).get('entity').get('scoring_url')\nprint scoring_endpoint\n\nprint '[\"%s\"]' % '\", \"'.join(map(str, stringIndexer_label.labels))", "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python2-spark20", "language": "python", "display_name": "Python 2 with Spark 2.0"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "name": "python", "pygments_lexer": "ipython2", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "version": "2.7.11"}}}